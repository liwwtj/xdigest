"""
Twitter æ•°æ®æŠ“å–æ¨¡å—
æ”¯æŒ Cookies ç™»å½•æ¨¡å¼æŠ“å–æ¨æ–‡
"""

import asyncio
from typing import List, Dict
from pathlib import Path
from datetime import datetime, timedelta, timezone
import logging
from twikit import Client

logger = logging.getLogger(__name__)


class TwitterFetcher:
    """Twitter æ¨æ–‡æŠ“å–å™¨"""

    def __init__(self, request_delay: int = 5, proxy: str = None, cookies_file: str = None,
                 retry_on_rate_limit: bool = True, max_retries: int = 3, max_tweet_age_hours: int = 9,
                 enable_thread_merging: bool = True, max_thread_fetches: int = 3):
        self.proxy = proxy
        self.cookies_file = cookies_file or "cookies.json"
        self.client = Client(language='en-US', proxy=proxy) if proxy else Client(language='en-US')
        self.request_delay = request_delay
        self.retry_on_rate_limit = retry_on_rate_limit
        self.max_retries = max_retries
        self.max_tweet_age_hours = max_tweet_age_hours
        self.enable_thread_merging = enable_thread_merging
        self.max_thread_fetches = max_thread_fetches
        self.stats = {
            'total_accounts': 0,
            'successful_accounts': 0,
            'failed_accounts': 0,
            'total_tweets': 0,
            'filtered_old_tweets': 0,
            'threads_detected': 0,
            'errors': []
        }

    async def init(self):
        """åˆå§‹åŒ– Clientï¼ˆåŠ è½½ Cookiesï¼‰"""
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ– Twitter Client...")
            if not Path(self.cookies_file).exists():
                logger.error(f"âŒ Cookies æ–‡ä»¶ä¸å­˜åœ¨: {self.cookies_file}")
                return False
            self.client.load_cookies(self.cookies_file)
            logger.info("âœ… Cookies åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ Client åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def get_following(self, username: str) -> List[str]:
        """è·å–æŒ‡å®šç”¨æˆ·çš„å…³æ³¨åˆ—è¡¨"""
        user = await self.client.get_user_by_screen_name(username)
        following = await self.client.get_user_following(user.id, count=200)
        return [u.screen_name for u in following]

    async def get_user_tweets(self, username: str, count: int = 5) -> List[Dict]:
        """è·å–æŒ‡å®šç”¨æˆ·çš„æœ€æ–°æ¨æ–‡ï¼ˆå¸¦é‡è¯•æœºåˆ¶å’Œæ—¶é—´è¿‡æ»¤ï¼‰"""
        cutoff_date = datetime.now(timezone.utc) - timedelta(hours=self.max_tweet_age_hours)

        for attempt in range(self.max_retries + 1):
            try:
                logger.info(f"ğŸ“¡ æŠ“å– @{username} çš„æ¨æ–‡...")
                user = await self.client.get_user_by_screen_name(username)
                tweets = await user.get_tweets('Tweets', count=count)

                results = []
                filtered_count = 0
                for tweet in tweets:
                    if hasattr(tweet, 'retweeted_tweet') and tweet.retweeted_tweet:
                        continue

                    # è§£ææ—¶é—´
                    tweet_time = None
                    if hasattr(tweet, 'created_at') and tweet.created_at:
                        raw_time = tweet.created_at
                        # å¤„ç†ä¸åŒç±»å‹
                        if isinstance(raw_time, datetime):
                            tweet_time = raw_time if raw_time.tzinfo else raw_time.replace(tzinfo=timezone.utc)
                        elif isinstance(raw_time, str):
                            try:
                                # Twitter API è¿”å›æ ¼å¼: "Wed Oct 10 20:19:24 +0000 2018"
                                tweet_time = datetime.strptime(raw_time, "%a %b %d %H:%M:%S %z %Y")
                            except ValueError:
                                try:
                                    # å¤‡ç”¨æ ¼å¼: "2026-01-30 12:34:56" (å‡å®š UTC)
                                    tweet_time = datetime.strptime(raw_time[:19], "%Y-%m-%d %H:%M:%S").replace(tzinfo=timezone.utc)
                                except ValueError:
                                    tweet_time = None

                        # æ—¶é—´è¿‡æ»¤
                        if tweet_time and tweet_time < cutoff_date:
                            filtered_count += 1
                            continue

                    # æ ¼å¼åŒ–æ—¶é—´ä¸ºåŒ—äº¬æ—¶é—´æ˜¾ç¤º
                    if tweet_time:
                        beijing_time = tweet_time.astimezone(timezone(timedelta(hours=8)))
                        created_at_str = beijing_time.strftime("%Y-%m-%d %H:%M")
                    else:
                        created_at_str = ''

                    tweet_data = {
                        'username': username,
                        'text': tweet.text,
                        'created_at': created_at_str,
                        'likes': tweet.favorite_count if hasattr(tweet, 'favorite_count') else 0,
                        'retweets': tweet.retweet_count if hasattr(tweet, 'retweet_count') else 0,
                        'url': f'https://twitter.com/{username}/status/{tweet.id}'
                    }
                    results.append(tweet_data)

                if filtered_count > 0:
                    logger.info(f"   è¿‡æ»¤äº† {filtered_count} æ¡è¶…è¿‡ {self.max_tweet_age_hours} å°æ—¶çš„æ—§æ¨æ–‡")
                    self.stats['filtered_old_tweets'] += filtered_count

                # Thread æ£€æµ‹ä¸åˆå¹¶
                if self.enable_thread_merging and results:
                    results = await self._merge_threads(username, results, tweets)

                logger.info(f"âœ… @{username}: æˆåŠŸè·å– {len(results)} æ¡æ¨æ–‡")
                self.stats['successful_accounts'] += 1
                self.stats['total_tweets'] += len(results)
                return results

            except Exception as e:
                error_msg = str(e)
                is_rate_limit = '429' in error_msg or 'Rate limit' in error_msg

                if is_rate_limit and self.retry_on_rate_limit and attempt < self.max_retries:
                    wait_time = (attempt + 1) * 30  # 30s, 60s, 90s
                    logger.warning(f"âš ï¸ @{username} è§¦å‘é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time}s åé‡è¯• ({attempt+1}/{self.max_retries})...")
                    await asyncio.sleep(wait_time)
                    continue

                logger.warning(f"âš ï¸ æŠ“å–å¤±è´¥ - @{username}: {error_msg}")
                self.stats['failed_accounts'] += 1
                self.stats['errors'].append(f"@{username}: {error_msg}")
                return []

        return []

    async def _merge_threads(self, username: str, results: List[Dict], raw_tweets) -> List[Dict]:
        """æ£€æµ‹è‡ªå›å¤ thread å¹¶åˆå¹¶ä¸ºå•æ¡æ¨æ–‡"""
        # å»ºç«‹ tweet id -> raw tweet çš„æ˜ å°„
        tweet_map = {}
        for tweet in raw_tweets:
            tweet_map[tweet.id] = tweet

        # æ‰¾å‡ºè‡ªå›å¤æ¨æ–‡ï¼ˆreply_to çš„ user æ˜¯è‡ªå·±ï¼‰
        self_replies = set()
        reply_to_parent = {}  # child_id -> parent_id
        for tweet in raw_tweets:
            if not hasattr(tweet, 'in_reply_to_tweet_id') or not tweet.in_reply_to_tweet_id:
                continue
            # æ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå›å¤
            reply_to_user = None
            if hasattr(tweet, '_data') and isinstance(tweet._data, dict):
                legacy = tweet._data.get('legacy', {})
                reply_to_user = legacy.get('in_reply_to_user_id_str')
            if not reply_to_user and hasattr(tweet, 'user') and tweet.user:
                # fallback: å¦‚æœ parent åœ¨æœ¬æ‰¹æ¬¡ä¸­ä¸”åŒä¸€ç”¨æˆ·
                parent_id = tweet.in_reply_to_tweet_id
                if parent_id in tweet_map and tweet_map[parent_id].user.id == tweet.user.id:
                    reply_to_user = tweet.user.id
            if reply_to_user and hasattr(tweet, 'user') and tweet.user and str(reply_to_user) == str(tweet.user.id):
                self_replies.add(tweet.id)
                reply_to_parent[tweet.id] = tweet.in_reply_to_tweet_id

        if not self_replies:
            return results

        # æ‰¾åˆ° thread çš„æ ¹æ¨æ–‡ ID
        root_ids = set()
        for child_id, parent_id in reply_to_parent.items():
            # æ²¿ç€ parent é“¾å¾€ä¸Šæ‰¾æ ¹
            root = parent_id
            visited = {child_id}
            while root in reply_to_parent and root not in visited:
                visited.add(root)
                root = reply_to_parent[root]
            root_ids.add(root)

        logger.info(f"ğŸ§µ @{username}: æ£€æµ‹åˆ° {len(root_ids)} ä¸ª thread")

        # é€šè¿‡ API è·å–å®Œæ•´ thread
        thread_texts = {}  # root_id -> [texts]
        thread_fetches = 0
        for root_id in root_ids:
            if thread_fetches >= self.max_thread_fetches:
                logger.info(f"   å·²è¾¾åˆ°æœ€å¤§ thread è¯·æ±‚æ•° ({self.max_thread_fetches})ï¼Œè·³è¿‡å‰©ä½™")
                break
            try:
                root_tweet = await self.client.get_tweet_by_id(root_id)
                thread_fetches += 1
                await asyncio.sleep(self.request_delay)

                if hasattr(root_tweet, 'thread') and root_tweet.thread:
                    texts = [t.text for t in root_tweet.thread if hasattr(t, 'text')]
                    if texts:
                        thread_texts[root_id] = texts
                        self.stats['threads_detected'] += 1
                        logger.info(f"   ğŸ§µ è·å–åˆ° thread ({len(texts)} æ¡æ¨æ–‡)")
                        continue

                # fallback: ç”¨æœ¬æ‰¹æ¬¡ä¸­å·²æœ‰çš„æ¨æ–‡æ‹¼æ¥
                self._fallback_thread_from_batch(root_id, tweet_map, reply_to_parent, thread_texts)

            except Exception as e:
                logger.warning(f"   âš ï¸ è·å– thread {root_id} å¤±è´¥: {e}ï¼Œä½¿ç”¨æœ¬åœ° fallback")
                self._fallback_thread_from_batch(root_id, tweet_map, reply_to_parent, thread_texts)

        if not thread_texts:
            return results

        # åˆå¹¶ thread åˆ° results
        merged_ids = set()
        for root_id, texts in thread_texts.items():
            # æ”¶é›†å±äºè¿™ä¸ª thread çš„æ‰€æœ‰ tweet id
            chain_ids = {root_id}
            for child_id, parent_id in reply_to_parent.items():
                r = parent_id
                visited = {child_id}
                while r in reply_to_parent and r not in visited:
                    visited.add(r)
                    r = reply_to_parent[r]
                if r == root_id:
                    chain_ids.add(child_id)
            merged_ids.update(chain_ids)

        # é‡å»º resultsï¼šæ›¿æ¢ thread æ¨æ–‡ä¸ºåˆå¹¶ç‰ˆæœ¬
        new_results = []
        used_roots = set()
        for tweet_data in results:
            tweet_id = tweet_data['url'].split('/')[-1]  # ä» url æå– id
            if tweet_id in merged_ids:
                # æ‰¾åˆ°å¯¹åº”çš„ root
                root = tweet_id
                visited = set()
                while root in reply_to_parent and root not in visited:
                    visited.add(root)
                    root = reply_to_parent[root]
                if root in thread_texts and root not in used_roots:
                    used_roots.add(root)
                    merged_text = "\n---\n".join(thread_texts[root])
                    merged_data = dict(tweet_data)
                    merged_data['text'] = merged_text
                    merged_data['is_thread'] = True
                    merged_data['thread_length'] = len(thread_texts[root])
                    # ä½¿ç”¨ root æ¨æ–‡çš„ url
                    merged_data['url'] = f'https://twitter.com/{username}/status/{root}'
                    new_results.append(merged_data)
                # è·³è¿‡ thread ä¸­çš„å…¶ä»–æ¨æ–‡
                continue
            else:
                new_results.append(tweet_data)

        return new_results

    def _fallback_thread_from_batch(self, root_id, tweet_map, reply_to_parent, thread_texts):
        """ä»å½“å‰æ‰¹æ¬¡ä¸­æ‹¼æ¥ threadï¼ˆfallbackï¼‰"""
        chain = []
        # æ”¶é›† root + æ‰€æœ‰æŒ‡å‘ root çš„ children
        if root_id in tweet_map:
            chain.append((root_id, tweet_map[root_id]))
        for child_id, parent_id in reply_to_parent.items():
            r = parent_id
            visited = {child_id}
            while r in reply_to_parent and r not in visited:
                visited.add(r)
                r = reply_to_parent[r]
            if r == root_id and child_id in tweet_map:
                chain.append((child_id, tweet_map[child_id]))

        if len(chain) > 1:
            # æŒ‰æ—¶é—´æ’åº
            chain.sort(key=lambda x: x[1].created_at if hasattr(x[1], 'created_at') and x[1].created_at else '')
            texts = [t.text for _, t in chain if hasattr(t, 'text')]
            if texts:
                thread_texts[root_id] = texts
                self.stats['threads_detected'] += 1

    async def fetch_multiple_accounts(self, usernames: List[str], tweets_per_account: int = 5,
                                      concurrency: int = 2) -> List[Dict]:
        """æ‰¹é‡æŠ“å–å¤šä¸ªè´¦å·çš„æ¨æ–‡ï¼ˆå¹¶å‘ï¼‰"""
        self.stats['total_accounts'] = len(usernames)
        semaphore = asyncio.Semaphore(concurrency)
        results = [None] * len(usernames)

        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æŠ“å– {len(usernames)} ä¸ªè´¦å· (å¹¶å‘æ•°: {concurrency})...")

        async def _fetch(index, username):
            async with semaphore:
                logger.info(f"[{index+1}/{len(usernames)}] æŠ“å– @{username}...")
                tweets = await self.get_user_tweets(username, tweets_per_account)
                results[index] = tweets
                await asyncio.sleep(self.request_delay)

        await asyncio.gather(*[_fetch(i, u) for i, u in enumerate(usernames)])

        all_tweets = []
        for tweets in results:
            if tweets:
                all_tweets.extend(tweets)

        logger.info(f"ğŸ“Š æ‰¹é‡æŠ“å–å®Œæˆ!")
        logger.info(f"   - æ€»è´¦å·æ•°: {self.stats['total_accounts']}")
        logger.info(f"   - æˆåŠŸ: {self.stats['successful_accounts']}")
        logger.info(f"   - å¤±è´¥: {self.stats['failed_accounts']}")
        logger.info(f"   - æ€»æ¨æ–‡æ•°: {self.stats['total_tweets']}")
        if self.stats['total_accounts'] > 0:
            logger.info(f"   - æˆåŠŸç‡: {self.stats['successful_accounts']/self.stats['total_accounts']*100:.1f}%")

        return all_tweets

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats


async def test_fetcher():
    """æµ‹è¯•å‡½æ•°"""
    fetcher = TwitterFetcher(proxy="http://127.0.0.1:7890")
    if not await fetcher.init():
        print("åˆå§‹åŒ–å¤±è´¥")
        return

    test_accounts = ['sama', 'karpathy']
    tweets = await fetcher.fetch_multiple_accounts(test_accounts, tweets_per_account=3)

    print(f"\næŠ“å–åˆ° {len(tweets)} æ¡æ¨æ–‡:")
    for tweet in tweets[:5]:
        print(f"\n@{tweet['username']}: {tweet['text'][:100]}...")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    asyncio.run(test_fetcher())
